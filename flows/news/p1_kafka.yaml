from: local/db_envs.yaml

name: load and extract news from web page
description: 接收采集平台通过Kafka推送的网页数据，进行标题、时间、作者、正文等抽取

#loader: database.kafka.WebConsumer(**kafka, topics=['gdelt'])
loader: database.kafka.TimedConsumer(**kafka, topics=['gdelt'], max_wait_times=15)
#loader: JsonLine('data/news2.jsonl')

nodes:
  count: Count(ticks=10)
  print: Print('id', 'url')
  select: Select('id', 'url', 'html', 'event_time_date', 'mention_time_date')
  extract: nlp.news.Constor('http://10.60.1.145:7100/constor/process', key='html')
#  extract: nlp.news.Extract(key='html', target_key='content')
  flat: FlatProperty('html', inherit_props=True)
  filter: FieldsExist('publish_time')
  min_time: MinValue('publish_time', 'publish_time', 'event_time_date', 'mention_time_date')
  select1: "=lambda r: r['publish_time'] if r['publish_time']<r['event_time_date']-86400000 else r['event_time_date']"
  select2: "=lambda r: r['publish_time'] if r['publish_time']<r['mention_time_date']-86400000 else r['mention_time_date']"
  map_time1: Map(select1, key='publish_time')
  map_time2: Map(select2, key='publish_time')
#  chain: Chain(select, extract, flat, filter, min_time)
  chain: Chain(select, extract, flat, filter)

#processor: Chain(chain, Select('id', 'title', 'origin_publish_time', 'url'), print)
processor: Chain(chain, WriteJson('data/news3.jsonl', buffer_size=10), count)
