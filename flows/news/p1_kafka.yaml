from: local/db_envs.yaml

name: load and extract news from web page
description: 接收采集平台通过Kafka推送的网页数据，进行标题、时间、作者、正文等抽取

consts:
  es1:
    index: goinv3_document_news

#loader: database.kafka.WebConsumer(**kafka, topics=['gdelt'])
loader: database.kafka.TimedConsumer(**kafka, topics=['gdelt'], max_wait_times=15)
#loader: JsonLine('data/news2.jsonl')

nodes:
  db_es: util.database.elasticsearch.ES(**es1)
  dedup: DistinctByDatabase(db_es, key='id')

  count: Count(ticks=10)
  print: Print('id', 'url')
  select: Select('id', 'url', 'html', 'event_time_date', 'mention_time_date')
  backup: WriteJsonScroll('data/news/news')
  extract: nlp.news.Constor('http://10.60.1.145:7100/constor/process', key='html', target_key='info')
#  extract: nlp.news.Extract(key='html', target_key='content')
  flat: FlatProperty('info', inherit_props=True)
#  valid_time: "=lambda r: r['publish_time'] < r['event_time_date'] + 86400000 if r.get('publish_time') else False"
  valid_time: "=lambda r:  True if r.get('publish_time') else False"
  backup_bad: WriteJsonIf(valid_time, 'data/news/bad', scroll=100)
  rm_html: RemoveFields('html')
  chain: Chain(dedup, select, extract, FieldsNonEmpty('info'), flat, backup_bad, FilterByTime(key='publish_time'), rm_html)

processor: Chain(chain, Print(), count)
