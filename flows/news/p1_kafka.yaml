from: local/db_envs.yaml

name: load and extract news from web page
description: 接收采集平台通过Kafka推送的网页数据，进行标题、时间、作者、正文等抽取

consts:
  es1:
    index: goinv3_document_news

loader: Function('wikidata_filter.gestata.dbops.scroll', db_kafka)
#loader: JsonLine('data/news2.jsonl')

nodes:
  db_es: util.database.elasticsearch.ES(**es1)
# util.database.kafka_rest.WebConsumer(**kafka, topics=['gdelt'])
  db_kafka: util.database.kafka_rest.TimedConsumer(**kafka, topics=['gdelt'], max_wait_times=15)
  dedup: DistinctByDatabase(db_es, key='id')

  count: Count(ticks=10)
  print: Print('id', 'url')
  select: Select('id', 'url', 'html', 'event_time_date', 'mention_time_date')
  backup: WriteJsonScroll('data/news/news')
  extract: Map('gestata.news.constor_extract', key='html', target_key='info', api_base='http://10.60.1.145:7100/constor/process')
  flat: FlatProperty('info', inherit_props=True)
#  valid_time: "=lambda r: r['publish_time'] < r['event_time_date'] + 86400000 if r.get('publish_time') else False"
  valid_time: "=lambda r:  True if r.get('publish_time') else False"
  backup_bad: WriteJsonIf(valid_time, 'data/news/bad', scroll=100)
  rm_html: RemoveFields('html')
  chain: Chain(dedup, select, extract, FieldsNonEmpty('info'), flat, backup_bad, FilterByTime(key='publish_time'), rm_html)

processor: Chain(chain, Print(), count)
