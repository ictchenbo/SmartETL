from: flows/news/p1_kafka.yaml

name: translate and vectorize
description: 对新闻标题和正文基于大模型翻译，向量化并写入向量库
consts:
  qdrant:
    collection: chunk_news_v2
  es1:
    index: goinv3_document_news
  chatglm4: http://10.208.63.29:8888
  bge_large_zh: http://10.208.63.29:8001/embed

nodes:
  prompt: util.files.text('config/prompt/news_translate.txt')
  prompt1: util.files.text('config/prompt/news_translate_abs.txt')
  count_es: Count(ticks=10, label='Write-ES')
  count_qd: Count(ticks=10, label='Write-Qdrant')

  map: Map(int, key='id')
  add_ts: AddTS('inserted_time')
  chain2: Chain(map, add_ts)

  rename: RenameFields(title='title_origin', content='content_origin')
  trans_title: model.GoGPT(api_base=chatglm4, key='title_origin', prompt=prompt, target_key='title')
  translate: model.GoGPT(api_base=chatglm4, key='content_origin', prompt=prompt1, target_key='content')

  write_es: database.ESWriter(**es1, buffer_size=1)
  vector: model.embed.Local(api_base=bge_large_zh, key='content', target_key='vector')
  write_qd: database.qdrant.Qdrant(**qdrant, buffer_size=1)
  fork: Fork(Chain(write_es, count_es), Chain(vector, write_qd, count_qd), copy_data=True)

  chain3: Chain(rename, trans_title, translate, fork)

processor: Chain(chain, Print('id', 'title', 'origin_publish_time', 'url'), chain2, chain3)
