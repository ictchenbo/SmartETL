name: arxiv论文HTML和PDF下载
description: 下载数据集中arxiv论文html,pdf格式，并保存在data/arxiv_html,arxiv_pdf下

consts:
  kafka_host: 10.60.1.148:9092

loader: Function('wikidata_filter.gestata.dbops.scroll', kafka1)
nodes:
  kafka1: util.database.kafka.Kafka(host=kafka_host, topic='arxiv_gather')
  kafka2: util.database.kafka.Kafka(host=kafka_host, topic='arxiv_parse2')
  #拼接html下载url
  search: Map('gestata.arxiv.url4html',key='id', target_key='url_html')
  #下载html内容
  download: Map('util.http.content', key='url_html', target_key='content', most_times=3, ignore_error=True)
  #元数据集写入kafka
  write_kafka: Function('wikidata_filter.gestata.dbops.upsert', kafka2)
  #保存html文件
  save_file: WriteFiles('data/arxiv_html', name_key='id', suffix='.html')
  #拼接pdf下载url
  search_pdf: Map('gestata.arxiv.url4pdf',key='id', target_key='url_pdf')
  #下载pdf内容
  download_pdf: Map('util.http.content', key='url_pdf', target_key='pdf_content', most_times=3, ignore_error=True)
  #保存pdf文件
  save_pdf_file: WriteFiles('data/arxiv_pdf', name_key='id',content_key='pdf_content', suffix='.pdf')
  #记录处理了多少个数据
  write_count: gestata.arxiv.WriteCount('data/arxiv_download_count2.txt')
  chain1: Chain(search, download, save_file)
  chain2: Chain(search_pdf,download_pdf,save_pdf_file)
  #并行执行两个流程
  fork: Fork(chain1, chain2)

#processor: Chain(Print("id"),write_count, fork)
processor: Chain(Print("id"),write_count,write_kafka,search_pdf,download_pdf,save_pdf_file)
