name: process news from mongodb
description: 从MongoDB轮询读取新闻，进行主题分类、地点识别，建立ES全文索引和Qdrant向量化索引。ES索引mapping查看`config/es-mappings/news.json`
arguments: 0
consts:
  mongo1:
    host: 10.60.1.145
    database: global-news
    collection: news
  es_config:
    host: 10.208.61.117
    port: 9200
    index: news
    buffer_size: 10
  qdrant_config:
    host: 10.60.1.145
    port: 6333
    collection: chunk
  chatglm4: http://10.208.63.29:8888
  bge_large_zh: http://10.208.63.29:8001/embed
  renames:
    content.title: title
    content.content: content
    content.date: date
    event_time_date: publish_time

nodes:
  loader: database.MongoLoader(**mongo1, sortby="_id", limit=1000)

  prompt1: util.files.text('config/prompt/news_classify.txt')
  prompt2: util.files.text('config/prompt/news_country_rec.txt')

  select: Select('id', 'content.title', 'content.content', 'url', 'event_time_date')
  rename: RenameFields(**renames)
  concat: ConcatFields('cont', 'title', 'content', sep='\n')
  short_content: Map(lambda s:s[:200], 'cont', target_key='short_content')
  remove: RemoveFields('cont')
  chain1: Chain(select, rename, concat, short_content, remove)

  class1: model.GoGPT(api_base=chatglm4, field='short_content', prompt=prompt1, target_key='tag1')
  class2: model.GoGPT(api_base=chatglm4, field='short_content', prompt=prompt2, target_key='tag2')

  remove2: RemoveFields('short_content')
  tags: nlp.splitter.TagSplit('tag1', 'tag2')
  writer: database.ESWriter(**es_config)

  counter: Count(ticks=10)

  chunk: nlp.splitter.TextSplit(key='content',target_key='chunks')
  flat: Flat(key='chunks', inherit_props=True)
  vector: model.embed.Local(api_base=bge_large_zh, field='chunks', target_key='vector')
  write_qd: database.Qdrant(**qdrant_config)
  write2: Chain(chunk, flat, vector, write_qd)

loader: TimedLoader(loader, interval=3600)
processor: Chain(chain1, class1, class2, remove2, tags, writer, write2, counter)
